{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC-515 Predictive Modeling for Medical Costs\n",
    "\n",
    "### Overview \n",
    "This project focuses on predicting individual medical costs using linear regression models built from the Medical Cost Personal Dataset. This notebook will explore and prepare the data, engineer meaningful features, and apply multiple regression techniques, including baseline, Ridge, and Lasso, to evaluate model performance. Finally, this notebook will compare results, interpret feature importance, and discuss how these insights can support better decision‑making in health‑care settings.\n",
    "\n",
    "#### Task 1. Data Processing and Feature Engineering\n",
    "Load and explore the dataset, address missing values, summarize patterns with visuals and statistics, and note key observations and assumptions while identifying predictors from the medical cost distribution. Create and justify new features, and apply normalization or standardization only where needed based on model requirements.\n",
    "\n",
    "#### Task 2. Modeling and Evaluation\n",
    "Implement a baseline linear regression model using your selected features, split the data into training and testing sets, and compare it with Ridge and Lasso regression while evaluating each through residual checks, diagnostics, and performance metrics. Train the validated models, generate predictions, and assess results using measures such as RMSE, MAE, and R‑squared to guide your interpretation.\n",
    "\n",
    "#### Task 3. Analysis and Interpretation\n",
    "Compare model performance using tables and plots, noting any unexpected patterns or errors, and evaluate feature importance to determine which predictors most strongly influence medical cost estimates. Use these insights to recommend how health‑care providers can apply the predictions and key features to improve patient care and resource planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Data Processing and Feature Engineering\n",
    "The dataset of interest is the Medical Cost Personal Datasets, which contains information about individuals' medical costs and various demographic and health-related features. This dataset will be the foundation for our predictive modeling efforts.\n",
    "\n",
    "To begin, import the necessary Python packages for data exploration, visualization, and building regression models, as well as load the dataset into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset directly (no file path needed)\n",
    "path = kagglehub.dataset_download(\"mirichoi0218/insurance\")\n",
    "\n",
    "# Load the CSV\n",
    "df_medical = pd.read_csv(path + \"/insurance.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df_medical.head())\n",
    "\n",
    "# Check if data types are appropriate\n",
    "print(df_medical.info()) # May need to convert boolean columns to numeric later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pair of duplicate rows was identified and removed during the cleaning process. Although those could have been unique patient records, they were treated as duplicates for the sake of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(f\"Before removing duplicates: {df_medical.shape[0]} rows\")\n",
    "df_medical.duplicated().any()  # 1 pair of duplicate rows found\n",
    "\n",
    "# Save duplicate rows if any exist\n",
    "duplicate_rows = df_medical[df_medical.duplicated(keep=False)]\n",
    "print(f\"Duplicate rows found:\\n{duplicate_rows}\")\n",
    "\n",
    "# There appears to be one duplicate entry. \n",
    "# Create a final dataframe without duplicates\n",
    "df_medical_final = df_medical.drop_duplicates()\n",
    "\n",
    "# Confirm removal of duplicates\n",
    "print(f\"After removing duplicates: {df_medical_final.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final cleaning steps involves converting boolean columns to integers ('sex' and 'smoker'), and creating a new feature by converting the continuous age variable into categorical age‑range groups. These categories align with the World Health Organization’s (WHO) adult age‑classification framework, which defines major life‑stage groups such as young adults (25–44), middle‑aged adults (44–60), and older adults (60+). Using WHO‑based groupings provides a standardized, globally recognized structure for demographic analysis and supports clearer interpretation in predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'sex' and 'smoker' to numeric data types.\n",
    "df_medical_final['sex'] = df_medical_final['sex'].replace({'male': 1, 'female': 0}).astype(int)\n",
    "df_medical_final['smoker'] = df_medical_final['smoker'].replace({'yes': 1, 'no': 0}).astype(int)\n",
    "# Check the updated data types\n",
    "print(df_medical_final.dtypes[['sex', 'smoker']])\n",
    "\n",
    "# Convert 'age' to ranges based on WHO adult age classifications.\n",
    "age_bins = [18, 25, 45, 60, 100]\n",
    "age_labels = ['18-24', '25-44', '45-59', '60+']\n",
    "\n",
    "df_medical_final.loc[:, 'age_group'] = pd.cut(\n",
    "    df_medical_final['age'],\n",
    "    bins=age_bins,\n",
    "    labels=age_labels,\n",
    "    right=False\n",
    ")\n",
    "\n",
    "# Check the new 'age_group' feature.\n",
    "print(df_medical_final.loc[:, ['age', 'age_group']].head(10))\n",
    "\n",
    "# Check the distribution of the new 'age_group' feature.\n",
    "print(df_medical_final.loc[:, 'age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Age', 'sex', 'bmi', 'children', and 'charges' are numeric features in the dataset. Although the description of 'charges' is somewhat vague, this variable represents the individual medical costs we aim to predict. Individuals range from 18 to 64 years old, with a mean age of about 39. 'Bmi' spans roughly 16 to 53 and shows moderate right skew, while 'charges' display substantial variability and a strong right skew due to a small number of high‑cost cases. The number of 'children' is generally low, with most individuals reporting 0 to 2 children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns.\n",
    "print(df_medical_final.describe())\n",
    "\n",
    "# Do men or women have more children on average?\n",
    "avg_children_by_sex = df_medical_final.groupby('sex')['children'].mean()\n",
    "print(f\"Average number of children by sex:\\n{avg_children_by_sex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining features, 'smoker' and 'region', are categorical variables. The dataset is nearly evenly split across the two sex categories, with 676 males and 662 females, and both groups have a similar average number of children. Smoker status is highly imbalanced, with 1,064 non‑smokers and only 274 smokers. All four geographic regions are represented, with the Southeast appearing most frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for categorical columns.\n",
    "print(df_medical_final.describe(include=['object']))\n",
    "\n",
    "# What are all unique values for region?\n",
    "print(df_medical_final['region'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we review the distribution of the target variable, charges, to assess its skewness and identify any potential outliers. The numerical summary shows that charges are right‑skewed, as the mean is noticeably higher than the median. This pattern indicates that a small subset of individuals incur exceptionally high medical costs, creating a long upper tail and potential outliers. The histogram below will further illustrate this skewed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of charges\n",
    "fig, ax = subplots(figsize=(8, 6))\n",
    "ax.hist(df_medical_final.loc[:, 'charges'], bins=30, color='slategray', edgecolor='black')\n",
    "ax.set_title('Distribution of Medical Charges')\n",
    "ax.set_xlabel('Charges')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix below shows the correlation coefficients between numerical features in the dataset. The strongest correlation to the target variable 'charges' is with 'smoker', 'sex', and 'age', indicating that these features may be important predictors of medical charges. 'Bmi' also shows a modest positive relationship with 'charges', suggesting it may contribute to cost variation even if its influence is weaker. Other variables, such as 'number of children', exhibit only minimal correlation, implying they are unlikely to play a substantial role in predicting medical expenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for numerical features.\n",
    "df_medical_final.info()  # Confirm which columns are numeric\n",
    "\n",
    "# Compute correlation matrix for numeric features only\n",
    "print(df_medical_final.select_dtypes(include='number').corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Modeling and Evaluation\n",
    "The next task will initiate the modeling process, implementing a baseline linear regression model using the predictor features selected during Task 1. These predictors were chosen based on their correlation with the target variable. After defining the feature matrix predictors (X) and the target feature (y), the dataset was split into training and testing subsets to ensure that model performance could be evaluated on unseen data. This split provides a fair assessment of how well the baseline model generalizes beyond the training sample.\n",
    "\n",
    "For the baseline model, the spread of points around the prediction line shows that although it captures the overall trend, its precision varies across individual cases. The baseline regression achieved an R² of about 0.80, meaning it explains a substantial portion of the variation in medical charges despite the noticeable prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = df_medical_final[['age', 'bmi', 'sex', 'smoker']]\n",
    "y = df_medical_final['charges']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42   # Research suggests using 42 as a standard random state for reproducibility\n",
    ") \n",
    "\n",
    "# Test the sizes of the splits\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Baseline Linear Regression Model\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate baseline model performance\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "mse_baseline = mean_squared_error(y_test, y_pred)\n",
    "r2_baseline = r2_score(y_test, y_pred)\n",
    "print(f\"Baseline Linear Regression Model - MSE: {mse_baseline}, R²: {r2_baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression (L2) is a regularization technique that adds a penalty equal to the square of the magnitude of coefficients to the loss function. This approach helps prevent overfitting by shrinking the coefficients of less important features, leading to a more generalized model. Ridge Regression is particularly useful when dealing with multicollinearity among predictors, as it can stabilize coefficient estimates. A common disadvantage of Ridge Regression is that it may not perform well when the true relationship between the features and the target variable is highly nonlinear.\n",
    "\n",
    "The L2 produced an R² of 0.80 and an MSE of roughly 36 million, showing that it explains a substantial portion of the variance in medical charges while maintaining prediction errors comparable to the baseline model. The predictions fall within a reasonable range, suggesting that the L2 penalty helped control coefficient magnitude without decreasing overall accuracy. These results show that Ridge provided a more regularized and reliable version of linear regression, particularly useful given the variability and skewness of medical‑cost data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "# Fit the model on the training data.\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set.\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "print(f\"Ridge Regression Predictions:\\n{y_pred[:10]}\")\n",
    "\n",
    "# Evaluate the Ridge Regression model\n",
    "mse_ridge = mean_squared_error(y_test, y_pred)\n",
    "r2_ridge = r2_score(y_test, y_pred)\n",
    "print(f\"Ridge Regression Mean Squared Error: {mse_ridge:.2f}\")\n",
    "print(f\"Ridge Regression R^2 Score: {r2_ridge:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression (L1) is another regularization technique that adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function. This approach can lead to sparse models where some feature coefficients are exactly zero, effectively performing feature selection. Lasso Regression is particularly useful when we suspect that many features are irrelevant or when we want a more interpretable model. However, a potential drawback is that it may not perform well when the true relationship is complex and involves many small effects.\n",
    "\n",
    "In this analysis, the Lasso model achieved an MSE of 35,834,592 and an R² of 0.805, indicating that it explained about 80% of the variance in the outcome. These results show that the model generalized well to unseen data and maintained stability in its predictions, reflecting the balance Lasso strikes between model simplicity and predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model on the training data.\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set.\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "print(f\"Lasso Regression Predictions:\\n{y_pred[:10]}\")\n",
    "\n",
    "# Evaluate Lasso model performance\n",
    "mse_lasso = mean_squared_error(y_test, y_pred)\n",
    "r2_lasso = r2_score(y_test, y_pred)\n",
    "print(f\"Lasso Regression Model - MSE: {mse_lasso}, R²: {r2_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models made almost the same predictions, which is why their residuals overlap so closely on the plot below. This tells you that the data has a strong linear pattern. The only small concern is that the residuals spread out more at higher predicted values, which makes sense, due to some patients having unusually high charges. Overall, the models appear stable, and there is no need for major adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_lin = baseline_model.predict(X_test)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "\n",
    "# Residuals\n",
    "res_lin = y_test - y_pred_lin\n",
    "res_ridge = y_test - y_pred_ridge\n",
    "res_lasso = y_test - y_pred_lasso\n",
    "\n",
    "# Diverging colors from a standard palette\n",
    "colors = [\"#3B4CC0\", \"#B5B5B5\", \"#B40426\"]  # blue → grey → red\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_pred_lin, res_lin, alpha=0.5, color=colors[0], label=\"Linear\")\n",
    "plt.scatter(y_pred_ridge, res_ridge, alpha=0.5, color=colors[1], label=\"Ridge\")\n",
    "plt.scatter(y_pred_lasso, res_lasso, alpha=0.5, color=colors[2], label=\"Lasso\")\n",
    "\n",
    "plt.axhline(0, color='grey')\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Predicted Values (All Models)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the validated models are trained on the training data and predictions are made on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model training on the entire training set\n",
    "baseline_model.fit(X_train, y_train)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "\n",
    "# Show first 10 predictions for each model\n",
    "print(\"Baseline Model Predictions:\", y_pred_baseline[:10])\n",
    "print(\"Ridge Model Predictions:\", y_pred_ridge[:10])\n",
    "print(\"Lasso Model Predictions:\", y_pred_lasso[:10])\n",
    "\n",
    "# Evaluate all models\n",
    "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "print(f\"Baseline Linear Regression Model - MSE: {mse_baseline:.2f}, R²: {r2_baseline:.2f}\")\n",
    "\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "print(f\"Ridge Linear Regression Model - MSE: {mse_ridge:.2f}, R²: {r2_ridge:.2f}\")\n",
    "\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "print(f\"Lasso Linear Regression Model - MSE: {mse_lasso:.2f}, R²: {r2_lasso:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Analysis and Interpretation\n",
    "The three models performed almost identically, with all MSE and R² values differing only in the fourth or fifth decimal place. This indicates that the baseline linear regression already captured the underlying relationships well, leaving little need for regularization to improve performance. Ridge regression performed slightly worse, suggesting that the coefficient penalty may have softened meaningful signal. Lasso matched the baseline model closely, showing that no major predictors were eliminated and that the overall error patterns remained stable across models. Residual behavior appeared consistent, with no evidence under- or over‑prediction at higher values. Higher charges naturally produce larger errors in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table to compare model performances\n",
    "model_performance = pd.DataFrame({\n",
    "    'Model': ['Baseline Linear Regression', 'Ridge Regression', 'Lasso Regression'],\n",
    "    'Mean Squared Error': [mse_baseline, mse_ridge, mse_lasso],\n",
    "    'R^2 Score': [r2_baseline, r2_ridge, r2_lasso]\n",
    "})\n",
    "print(model_performance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in the analysis it was determined that smoker status is the strongest predictor of healthcare costs, and the model coefficients reinforce this conclusion. Across all three regression models, the smoker coefficient is consistently around $23,000, indicating a substantial increase in predicted charges for smokers compared to non‑smokers. In contrast, age and BMI have much smaller positive effects, and sex contributes very little, which aligns with their weaker correlations with the target. These results make sense given known cost drivers in healthcare and support smoking is a significant factor in determining healthcare costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Linear Regression coefficients\n",
    "print(\"Baseline Coefficients:\")\n",
    "print(pd.Series(baseline_model.coef_, index=X.columns))\n",
    "\n",
    "# Ridge Regression coefficients\n",
    "print(\"\\nRidge Coefficients:\")\n",
    "print(pd.Series(ridge_model.coef_, index=X.columns))\n",
    "\n",
    "# Lasso Regression coefficients\n",
    "print(\"\\nLasso Coefficients:\")\n",
    "print(pd.Series(lasso_model.coef_, index=X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "These predictions have real‑world value for healthcare cost management because they highlight which patient groups drive spending. Smoking status remains the strongest predictor, but age also plays a meaningful role. The dataset shows that the largest portion of patients fall into the 25–44 age group (531), followed by 45–59 (415), 18–24 (277), and 60+ (114). Understanding both the size of each age group and the model’s finding that charges increase by roughly $250 per year of age can help healthcare organizations anticipate where costs are likely to increase. This allows providers to target interventions or allocate more resources that reflect the actual distribution of their patient population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Choi, M. (2016). Medical Cost Personal Datasets [Data set]. Kaggle. https://www.kaggle.com/datasets/mirichoi0218/insurance  \n",
    "\n",
    "HealthXWire. (2025). WHO categories of age: Understanding global health definitions. https://wis.it.com/who-categories-of-age-global-health-definitions\n",
    "\n",
    "James, G., Witten, D., Hastie, T., Tibshirani, R., Taylor, J., & Guo, W. (2023). An introduction to statistical learning: With applications in Python. Springer.\n",
    "\n",
    "Koivunen-Niemi, L., & K. (2022). Learn best practices for color use in data visualization with python and data from our world in data (2018). In Sage Research Methods: Data Visualization. SAGE Publications, Ltd. https://doi.org/10.4135/9781529605198\n",
    "\n",
    "Microsoft. (2024). Copilot [Large language model]. Microsoft. https://copilot.microsoft.com\n",
    "[^1]: Copilot was used only for debugging code and refining wording; all analysis and modeling were performed independently.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
